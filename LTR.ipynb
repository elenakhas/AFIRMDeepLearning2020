{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Learning to Rank (LTR)\n",
    "\n",
    "\n",
    "### Include required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "import csv\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "def print_message(s):\n",
    "    print(\"[{}] {}\".format(datetime.datetime.utcnow().strftime(\"%b %d, %H:%M:%S\"), s), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train and test data readers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataUtils:\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_line(line):\n",
    "        tokens                              = line.strip().split(' ')\n",
    "        qid                                 = -1\n",
    "        feat                                = []\n",
    "        label                               = int(tokens[0])\n",
    "        for i in range(FEAT_COUNT):\n",
    "            feat.append(0)\n",
    "        for i in range(1, len(tokens)):\n",
    "            sub_tokens                      = tokens[i].split(':')\n",
    "            if sub_tokens[0] == 'qid':\n",
    "                qid                         = int(sub_tokens[1])\n",
    "            else:\n",
    "                feat_idx                    = int(sub_tokens[0])\n",
    "                feat_val                    = float(sub_tokens[1])\n",
    "                feat[feat_idx - 1]          = int(feat_val * FEAT_SCALE)\n",
    "        return qid, label, feat\n",
    "    \n",
    "    \n",
    "class DataReaderTrain():\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        self.data_file                      = data_file\n",
    "        self.__load_data(self.data_file)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.__allocate_minibatch()\n",
    "        return self\n",
    "\n",
    "    def __load_data(self, data_file):\n",
    "        self.data                           = {}\n",
    "        with open(data_file, mode='r', encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                qid, label, feat            = DataUtils.parse_line(line)\n",
    "                if qid not in self.data:\n",
    "                    self.data[qid]          = {}\n",
    "                if label not in self.data[qid]:\n",
    "                    self.data[qid][label]   = []\n",
    "                self.data[qid][label].append(feat)\n",
    "        self.data                           = {k: v for k, v in self.data.items() if len(v) > 1}\n",
    "        self.qids                           = list(self.data.keys())\n",
    "    \n",
    "    def __allocate_minibatch(self):\n",
    "        self.features                       = [np.zeros((MB_SIZE, FEAT_COUNT), dtype=np.float32) for i in range(2)]\n",
    "        self.labels                         = np.zeros((MB_SIZE), dtype=np.int64)\n",
    "        \n",
    "    def __clear_minibatch(self):\n",
    "        for i in range(2):\n",
    "            self.features[i].fill(np.float32(0))\n",
    "            \n",
    "    def __next__(self):\n",
    "        self.__clear_minibatch()\n",
    "        qids                                = random.sample(self.qids, MB_SIZE)\n",
    "        for i in range(MB_SIZE):\n",
    "            labels                          = random.sample(self.data[qids[i]].keys(), 2)\n",
    "            labels.sort(reverse=True)\n",
    "            for j in range(2):\n",
    "                feats                       = self.data[qids[i]][labels[j]]\n",
    "                feat                        = feats[random.randint(0, len(feats) - 1)]\n",
    "                for k in range(FEAT_COUNT):\n",
    "                    self.features[j][i, k]  = feat[k] / FEAT_SCALE\n",
    "        return [torch.from_numpy(self.features[i]).to(DEVICE) for i in range(2)], torch.from_numpy(self.labels).to(DEVICE)\n",
    "    \n",
    "    \n",
    "class DataReaderTest():\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        self.data_file                      = data_file\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.reader                         = open(self.data_file, mode='r', encoding=\"utf-8\")\n",
    "        self.__allocate_minibatch()\n",
    "        return self\n",
    "    \n",
    "    def __allocate_minibatch(self):\n",
    "        self.features                       = np.zeros((MB_SIZE, FEAT_COUNT), dtype=np.float32)\n",
    "        self.labels                         = np.zeros((MB_SIZE), dtype=np.int64)\n",
    "        \n",
    "    def __clear_minibatch(self):\n",
    "        self.features.fill(np.float32(0))\n",
    "            \n",
    "    def __next__(self):\n",
    "        self.__clear_minibatch()\n",
    "        qids                                = []\n",
    "        labels                              = []\n",
    "        cnt                                 = 0\n",
    "        for i in range(MB_SIZE):\n",
    "            line                            = self.reader.readline()\n",
    "            if line == '':\n",
    "                raise StopIteration\n",
    "                break\n",
    "            qid, label, feat                = DataUtils.parse_line(line)\n",
    "            qids.append(qid)\n",
    "            labels.append(label)\n",
    "            for j in range(FEAT_COUNT):\n",
    "                self.features[i, j]         = feat[j] / FEAT_SCALE\n",
    "            cnt                            += 1\n",
    "        return torch.from_numpy(self.features).to(DEVICE), qids, labels, cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        layers              = []\n",
    "        last_dim            = FEAT_COUNT\n",
    "        for i in range(NUM_HIDDEN_LAYERS):\n",
    "            layers.append(nn.Linear(last_dim, NUM_HIDDEN_NODES))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.LayerNorm(NUM_HIDDEN_NODES))\n",
    "            layers.append(nn.Dropout(p=DROPOUT_RATE))\n",
    "            last_dim        = NUM_HIDDEN_NODES\n",
    "        layers.append(nn.Linear(last_dim, 1))\n",
    "        layers.append(nn.ReLU())\n",
    "        self.model          = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x) * SCALE\n",
    "    \n",
    "    def parameter_count(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data paths and readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 11, 12:52:33] Starting\n",
      "[Feb 11, 12:54:28] Data loaded\n"
     ]
    }
   ],
   "source": [
    "print_message('Starting')\n",
    "DATA_DIR                    = 'data/'\n",
    "DATA_FILE_TRAIN             = os.path.join(DATA_DIR, 'train.txt')\n",
    "DATA_FILE_TEST              = os.path.join(DATA_DIR, 'vali.txt')\n",
    "MODEL_FILE                  = os.path.join(DATA_DIR, \"ltr.{}.dnn\")\n",
    "FEAT_COUNT                  = 136\n",
    "FEAT_SCALE                  = 1000\n",
    "MB_SIZE                     = 1024\n",
    "READER_TRAIN                = DataReaderTrain(DATA_FILE_TRAIN)\n",
    "READER_TRAIN_ITER           = iter(READER_TRAIN)\n",
    "print_message('Data loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 11, 13:06:22] Number of learnable parameters: 51457\n",
      "[Feb 11, 13:06:22] Learning rate: 0.0001\n",
      "[Feb 11, 13:07:08] epoch:0, loss: n/a, dcg: 3.8162324615654515, ndcg: 0.1868865435540341\n",
      "[Feb 11, 13:19:12] epoch:1, loss: 0.6793197426159168, dcg: 7.910645764532029, ndcg: 0.3423217080250453\n",
      "[Feb 11, 13:31:18] epoch:2, loss: 0.6266433053824585, dcg: 9.238024213276333, ndcg: 0.40259715804907337\n",
      "[Feb 11, 13:43:21] epoch:3, loss: 0.6122403577683144, dcg: 9.457329986816898, ndcg: 0.4094099645384927\n",
      "[Feb 11, 13:55:22] epoch:4, loss: 0.6060029697400751, dcg: 9.512703346287212, ndcg: 0.4109196603045744\n",
      "[Feb 11, 14:07:26] epoch:5, loss: 0.6018690334894927, dcg: 9.673141233725211, ndcg: 0.4153830232196682\n",
      "[Feb 11, 14:19:29] epoch:6, loss: 0.5990034446076606, dcg: 9.79821534699085, ndcg: 0.42145639651984124\n",
      "[Feb 11, 14:31:32] epoch:7, loss: 0.5969932790249004, dcg: 9.899844538968154, ndcg: 0.4275813586335423\n",
      "[Feb 11, 14:43:35] epoch:8, loss: 0.5952461467022658, dcg: 9.96184721487237, ndcg: 0.42707422382692517\n",
      "[Feb 11, 14:55:37] epoch:9, loss: 0.593663535451924, dcg: 9.845303097440892, ndcg: 0.424564673135237\n",
      "[Feb 11, 15:07:39] epoch:10, loss: 0.5921582516893977, dcg: 9.937993512795291, ndcg: 0.4252277398440451\n",
      "[Feb 11, 15:19:41] epoch:11, loss: 0.591426372047863, dcg: 9.988765679831255, ndcg: 0.4290788306307466\n",
      "[Feb 11, 15:31:45] epoch:12, loss: 0.5905008758054464, dcg: 9.96353594352108, ndcg: 0.427348546057977\n",
      "[Feb 11, 15:43:48] epoch:13, loss: 0.589783850038657, dcg: 9.95827802536695, ndcg: 0.426727345766082\n",
      "[Feb 11, 15:55:50] epoch:14, loss: 0.5889461355909589, dcg: 10.010086413100328, ndcg: 0.42934804765864126\n",
      "[Feb 11, 16:07:52] epoch:15, loss: 0.58812315762043, dcg: 10.017347706076766, ndcg: 0.4280893226860161\n",
      "[Feb 11, 16:19:53] epoch:16, loss: 0.5872858294751495, dcg: 9.964482565118407, ndcg: 0.4299726123720011\n",
      "[Feb 11, 16:31:57] epoch:17, loss: 0.586595413456962, dcg: 10.046946165924732, ndcg: 0.430770584307282\n",
      "[Feb 11, 16:43:59] epoch:18, loss: 0.5862406377564184, dcg: 10.003381136055129, ndcg: 0.429678952057651\n",
      "[Feb 11, 16:56:01] epoch:19, loss: 0.5854112188171712, dcg: 10.026609948914736, ndcg: 0.4295197301063823\n",
      "[Feb 11, 17:08:05] epoch:20, loss: 0.5851012473649462, dcg: 10.022141984879749, ndcg: 0.42856494835605313\n",
      "[Feb 11, 17:20:09] epoch:21, loss: 0.5845950752336648, dcg: 10.001814473147194, ndcg: 0.42719728990725836\n",
      "[Feb 11, 17:32:11] epoch:22, loss: 0.5840681260087877, dcg: 10.098081142933854, ndcg: 0.43291161861098093\n",
      "[Feb 11, 17:44:14] epoch:23, loss: 0.5831889699911699, dcg: 10.046466291588462, ndcg: 0.4320199449858192\n",
      "[Feb 11, 17:56:18] epoch:24, loss: 0.5830359210158349, dcg: 10.040924743402359, ndcg: 0.4318924864125834\n",
      "[Feb 11, 18:08:22] epoch:25, loss: 0.5823713854915695, dcg: 10.110963334838319, ndcg: 0.4330415575670898\n",
      "[Feb 11, 18:20:31] epoch:26, loss: 0.5820371925074141, dcg: 10.118623452834223, ndcg: 0.43243247572654203\n",
      "[Feb 11, 18:32:36] epoch:27, loss: 0.5817666279617697, dcg: 10.128602278513505, ndcg: 0.43268962947991224\n",
      "[Feb 11, 18:44:40] epoch:28, loss: 0.5815126555535244, dcg: 10.031258141917432, ndcg: 0.43324911401682653\n",
      "[Feb 11, 18:56:43] epoch:29, loss: 0.5813106095665717, dcg: 10.057849878867142, ndcg: 0.43137268510210947\n",
      "[Feb 11, 19:08:49] epoch:30, loss: 0.5812023040125496, dcg: 10.107149415669662, ndcg: 0.4346099277890401\n",
      "[Feb 11, 19:20:52] epoch:31, loss: 0.5808478316830588, dcg: 10.091242284905164, ndcg: 0.43302129747919044\n",
      "[Feb 11, 19:32:55] epoch:32, loss: 0.580037586078106, dcg: 10.141684932830122, ndcg: 0.4341012374178464\n",
      "[Feb 11, 19:32:56] Finished training\n"
     ]
    }
   ],
   "source": [
    "DEVICE                      = torch.device(\"cuda:1\")\n",
    "NUM_HIDDEN_NODES            = 128\n",
    "NUM_HIDDEN_LAYERS           = 3\n",
    "EPOCH_SIZE                  = 8192\n",
    "NUM_EPOCHS                  = 32\n",
    "LEARNING_RATE               = 0.0001\n",
    "DROPOUT_RATE                = 0.5\n",
    "SCALE                       = torch.tensor([1], dtype=torch.float).to(DEVICE)\n",
    "\n",
    "def train(net):\n",
    "    train_loss              = 0.0\n",
    "    net.train()\n",
    "    for mb_idx in range(EPOCH_SIZE):\n",
    "        features, labels    = next(READER_TRAIN_ITER)\n",
    "        out                 = torch.cat(tuple([net(features[i]) for i in range(2)]), 1)\n",
    "        loss                = criterion(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss         += loss.item()\n",
    "    return train_loss / EPOCH_SIZE\n",
    "\n",
    "def test(net, ep_idx, train_loss):\n",
    "    net.eval()\n",
    "    reader_test             = DataReaderTest(DATA_FILE_TEST)\n",
    "    reader_test_iter        = iter(reader_test)\n",
    "    results                 = {}\n",
    "    for features, qids, labels, cnt in reader_test_iter:\n",
    "        out                 = net(features).data.cpu()\n",
    "        row_cnt             = len(qids)\n",
    "        for i in range(row_cnt):\n",
    "            if qids[i] not in results:\n",
    "                results[qids[i]] = []\n",
    "            results[qids[i]].append((labels[i], out[i][0]))\n",
    "    avgndcg                 = 0\n",
    "    avgdcg                  = 0\n",
    "    for qid, docs in results.items():\n",
    "        dcg                 = 0\n",
    "        ranked              = sorted(docs, key=lambda x: x[1], reverse=True)\n",
    "        for i in range(min(10, len(ranked))):\n",
    "            rank            = i + 1\n",
    "            label           = ranked[i][0]\n",
    "            dcg            += ((2**label - 1) / math.log2(rank + 1))\n",
    "        idcg                = 0\n",
    "        ranked              = sorted(docs, key=lambda x: x[0], reverse=True)\n",
    "        for i in range(min(10, len(ranked))):\n",
    "            rank            = i + 1\n",
    "            label           = ranked[i][0]\n",
    "            idcg           += ((2**label - 1) / math.log2(rank + 1))\n",
    "        avgdcg             += dcg\n",
    "        if idcg > 0:\n",
    "            avgndcg        += (dcg / idcg)\n",
    "    avgdcg                 /= len(results)\n",
    "    avgndcg                /= len(results)\n",
    "    print_message('epoch:{}, loss: {}, dcg: {}, ndcg: {}'.format(ep_idx, train_loss, avgdcg, avgndcg))\n",
    "\n",
    "torch.manual_seed(1)\n",
    "net                         = DNN().to(DEVICE)\n",
    "criterion                   = nn.CrossEntropyLoss()\n",
    "optimizer                   = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "print_message('Number of learnable parameters: {}'.format(net.parameter_count()))\n",
    "print_message('Learning rate: {}'.format(LEARNING_RATE))\n",
    "test(net, 0, 'n/a')\n",
    "for ep_idx in range(NUM_EPOCHS):\n",
    "    train_loss              = train(net)\n",
    "    test(net, ep_idx + 1, str(train_loss))\n",
    "print_message('Finished training')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}