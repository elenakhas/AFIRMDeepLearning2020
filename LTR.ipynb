{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Learning to Rank (LTR)\n",
    "\n",
    "\n",
    "### Include required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import os.path\n",
    "import csv\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "def print_message(s):\n",
    "    print(\"[{}] {}\".format(datetime.datetime.utcnow().strftime(\"%b %d, %H:%M:%S\"), s), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train and test data readers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtils:\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_line(line):\n",
    "        tokens                              = line.strip().split(' ')\n",
    "        qid                                 = -1\n",
    "        feat                                = []\n",
    "        label                               = int(tokens[0])\n",
    "        for i in range(FEAT_COUNT):\n",
    "            feat.append(0)\n",
    "        for i in range(1, len(tokens)):\n",
    "            sub_tokens                      = tokens[i].split(':')\n",
    "            if sub_tokens[0] == 'qid':\n",
    "                qid                         = int(sub_tokens[1])\n",
    "            else:\n",
    "                feat_idx                    = int(sub_tokens[0])\n",
    "                feat_val                    = float(sub_tokens[1])\n",
    "                feat[feat_idx - 1]          = int(feat_val * FEAT_SCALE)\n",
    "        return qid, label, feat\n",
    "    \n",
    "    \n",
    "class DataReaderTrain():\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        self.data_file                      = data_file\n",
    "        self.__load_data(self.data_file)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.__allocate_minibatch()\n",
    "        return self\n",
    "\n",
    "    def __load_data(self, data_file):\n",
    "        self.data                           = {}\n",
    "        with open(data_file, mode='r', encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                qid, label, feat            = DataUtils.parse_line(line)\n",
    "                if qid not in self.data:\n",
    "                    self.data[qid]          = {}\n",
    "                if label not in self.data[qid]:\n",
    "                    self.data[qid][label]   = []\n",
    "                self.data[qid][label].append(feat)\n",
    "        self.data                           = {k: v for k, v in self.data.items() if len(v) > 1}\n",
    "        self.qids                           = list(self.data.keys())\n",
    "    \n",
    "    def __allocate_minibatch(self):\n",
    "        self.features                       = [np.zeros((MB_SIZE, FEAT_COUNT), dtype=np.float32) for i in range(2)]\n",
    "        self.labels                         = np.zeros((MB_SIZE), dtype=np.int64)\n",
    "        \n",
    "    def __clear_minibatch(self):\n",
    "        for i in range(2):\n",
    "            self.features[i].fill(np.float32(0))\n",
    "            \n",
    "    def __next__(self):\n",
    "        self.__clear_minibatch()\n",
    "        qids                                = random.sample(self.qids, MB_SIZE)\n",
    "        for i in range(MB_SIZE):\n",
    "            labels                          = random.sample(self.data[qids[i]].keys(), 2)\n",
    "            labels.sort(reverse=True)\n",
    "            for j in range(2):\n",
    "                feats                       = self.data[qids[i]][labels[j]]\n",
    "                feat                        = feats[random.randint(0, len(feats) - 1)]\n",
    "                for k in range(FEAT_COUNT):\n",
    "                    self.features[j][i, k]  = feat[k] / FEAT_SCALE\n",
    "        return [torch.from_numpy(self.features[i]).to(DEVICE) for i in range(2)], torch.from_numpy(self.labels).to(DEVICE)\n",
    "    \n",
    "    \n",
    "class DataReaderTest():\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        self.data_file                      = data_file\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.reader                         = open(self.data_file, mode='r', encoding=\"utf-8\")\n",
    "        self.__allocate_minibatch()\n",
    "        return self\n",
    "    \n",
    "    def __allocate_minibatch(self):\n",
    "        self.features                       = np.zeros((MB_SIZE, FEAT_COUNT), dtype=np.float32)\n",
    "        self.labels                         = np.zeros((MB_SIZE), dtype=np.int64)\n",
    "        \n",
    "    def __clear_minibatch(self):\n",
    "        self.features.fill(np.float32(0))\n",
    "            \n",
    "    def __next__(self):\n",
    "        self.__clear_minibatch()\n",
    "        qids                                = []\n",
    "        labels                              = []\n",
    "        cnt                                 = 0\n",
    "        for i in range(MB_SIZE):\n",
    "            line                            = self.reader.readline()\n",
    "            if line == '':\n",
    "                raise StopIteration\n",
    "                break\n",
    "            qid, label, feat                = DataUtils.parse_line(line)\n",
    "            qids.append(qid)\n",
    "            labels.append(label)\n",
    "            for j in range(FEAT_COUNT):\n",
    "                self.features[i, j]         = feat[j] / FEAT_SCALE\n",
    "            cnt                            += 1\n",
    "        return torch.from_numpy(self.features).to(DEVICE), qids, labels, cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        layers              = []\n",
    "        last_dim            = FEAT_COUNT\n",
    "        for i in range(NUM_HIDDEN_LAYERS):\n",
    "            layers.append(nn.Linear(last_dim, NUM_HIDDEN_NODES))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.LayerNorm(NUM_HIDDEN_NODES))\n",
    "            layers.append(nn.Dropout(p=DROPOUT_RATE))\n",
    "            last_dim        = NUM_HIDDEN_NODES\n",
    "        layers.append(nn.Linear(last_dim, 1))\n",
    "        layers.append(nn.ReLU())\n",
    "        self.model          = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x) * SCALE\n",
    "    \n",
    "    def parameter_count(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data paths and readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 27, 07:57:17] Starting\n",
      "[Jan 27, 07:57:51] Data loaded\n"
     ]
    }
   ],
   "source": [
    "print_message('Starting')\n",
    "DATA_DIR                    = 'letor/'\n",
    "DATA_FILE_TRAIN             = os.path.join(DATA_DIR, 'train-small.txt')\n",
    "DATA_FILE_TEST              = os.path.join(DATA_DIR, 'test.txt')\n",
    "MODEL_FILE                  = os.path.join(DATA_DIR, \"ltr.{}.dnn\")\n",
    "FEAT_COUNT                  = 136\n",
    "FEAT_SCALE                  = 1000\n",
    "MB_SIZE                     = 1024\n",
    "READER_TRAIN                = DataReaderTrain(DATA_FILE_TRAIN)\n",
    "READER_TRAIN_ITER           = iter(READER_TRAIN)\n",
    "print_message('Data loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jan 27, 08:05:21] Number of learnable parameters: 51457\n",
      "[Jan 27, 08:05:21] Learning rate: 0.0001\n",
      "[Jan 27, 08:06:03] epoch:0, loss: n/a, dcg: 3.5303611055939688, ndcg: 0.1742251117653203\n"
     ]
    }
   ],
   "source": [
    "#DEVICE                      = torch.device(\"cuda:1\")\n",
    "DEVICE                      = torch.device(\"cpu\")\n",
    "NUM_HIDDEN_NODES            = 128\n",
    "NUM_HIDDEN_LAYERS           = 3\n",
    "EPOCH_SIZE                  = 8192\n",
    "NUM_EPOCHS                  = 32\n",
    "LEARNING_RATE               = 0.0001\n",
    "DROPOUT_RATE                = 0.5\n",
    "SCALE                       = torch.tensor([1], dtype=torch.float).to(DEVICE)\n",
    "\n",
    "def train(net):\n",
    "    train_loss              = 0.0\n",
    "    net.train()\n",
    "    for mb_idx in range(EPOCH_SIZE):\n",
    "        features, labels    = next(READER_TRAIN_ITER) #Read in a new mini-batch of data!\n",
    "        out                 = torch.cat(tuple([net(features[i]) for i in range(2)]), 1)\n",
    "        loss                = criterion(out, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss         += loss.item()\n",
    "    return train_loss / EPOCH_SIZE\n",
    "\n",
    "def test(net, ep_idx, train_loss):\n",
    "    net.eval()\n",
    "    reader_test             = DataReaderTest(DATA_FILE_TEST)\n",
    "    reader_test_iter        = iter(reader_test)\n",
    "    results                 = {}\n",
    "    for features, qids, labels, cnt in reader_test_iter:\n",
    "        out                 = net(features).data.cpu()\n",
    "        row_cnt             = len(qids)\n",
    "        for i in range(row_cnt):\n",
    "            if qids[i] not in results:\n",
    "                results[qids[i]] = []\n",
    "            results[qids[i]].append((labels[i], out[i][0]))\n",
    "    avgndcg                 = 0\n",
    "    avgdcg                  = 0\n",
    "    for qid, docs in results.items():\n",
    "        dcg                 = 0\n",
    "        ranked              = sorted(docs, key=lambda x: x[1], reverse=True)\n",
    "        for i in range(min(10, len(ranked))):\n",
    "            rank            = i + 1\n",
    "            label           = ranked[i][0]\n",
    "            dcg            += ((2**label - 1) / math.log2(rank + 1))\n",
    "        idcg                = 0\n",
    "        ranked              = sorted(docs, key=lambda x: x[0], reverse=True)\n",
    "        for i in range(min(10, len(ranked))):\n",
    "            rank            = i + 1\n",
    "            label           = ranked[i][0]\n",
    "            idcg           += ((2**label - 1) / math.log2(rank + 1))\n",
    "        avgdcg             += dcg\n",
    "        if idcg > 0:\n",
    "            avgndcg        += (dcg / idcg)\n",
    "    avgdcg                 /= len(results)\n",
    "    avgndcg                /= len(results)\n",
    "    print_message('epoch:{}, loss: {}, dcg: {}, ndcg: {}'.format(ep_idx, train_loss, avgdcg, avgndcg))\n",
    "\n",
    "torch.manual_seed(1)\n",
    "net                         = DNN().to(DEVICE)\n",
    "criterion                   = nn.CrossEntropyLoss()\n",
    "optimizer                   = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "print_message('Number of learnable parameters: {}'.format(net.parameter_count()))\n",
    "print_message('Learning rate: {}'.format(LEARNING_RATE))\n",
    "test(net, 0, 'n/a')\n",
    "for ep_idx in range(NUM_EPOCHS):\n",
    "    train_loss              = train(net)\n",
    "    test(net, ep_idx + 1, str(train_loss))\n",
    "print_message('Finished training')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
