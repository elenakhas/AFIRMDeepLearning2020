{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding up a Simple Neural Network from Scratch\n",
    "\n",
    "In this demo we will code up a simple multi-layer neural network with a single neuron per layer using (i) PyTorch and (ii) from scratch. Then we will demonstrate that both produce _identical_ outcome.\n",
    "\n",
    "\n",
    "## A simple DNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class OurLinear:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.weight = random.uniform(-1, 1)     # randomly initialize learnable weight\n",
    "        self.bias = random.uniform(-1, 1)       # randomly initialize learnable bias\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x                              # save the input for gradient computation later\n",
    "        out = x * self.weight + self.bias       # compute output\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad_out, lr):\n",
    "        grad_w = grad_out * self.x              # compute gradients w.r.t. weight\n",
    "        grad_b = grad_out                       # compute gradients w.r.t. bias\n",
    "        grad_in = grad_out * self.weight        # compute gradients w.r.t. input\n",
    "        self.weight -= lr * grad_w              # update weight\n",
    "        self.bias -= lr * grad_b                # update bias\n",
    "        return grad_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTanh:\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.y = np.tanh(x)                     # compute and save output for gradient computation later\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, grad_out, lr):\n",
    "        grad_in = grad_out * (1 - self.y**2)    # compute gradients w.r.t. input\n",
    "        return grad_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDNN:\n",
    "\n",
    "    def __init__(self, num_layers):\n",
    "        self.layers = []\n",
    "        for i in range(num_layers):                     # define a sequence of linear and non-linear layers\n",
    "            self.layers.append(OurLinear())\n",
    "            self.layers.append(OurTanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:                       # run layers sequentially\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, grad_out, lr):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_out = layer.backward(grad_out, lr)     # backpropagate gradients through the layers\n",
    "        return grad_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple DNN in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class PyDNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers):\n",
    "        super(PyDNN, self).__init__()\n",
    "        self.layers = []\n",
    "        for i in range(num_layers):                 # define a sequence of linear and non-linear layers\n",
    "            self.layers.append(nn.Linear(1, 1))\n",
    "            self.layers.append(nn.Tanh())\n",
    "        self.model = nn.Sequential(*self.layers)    # wrapping it for easier registration and invocation\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple mean squared loss from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurMSELoss:\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.z = (y - x)        # save the intermediate for gradient computation later\n",
    "        out = self.z**2         # compute output\n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        grad_in = -2 * self.z   # compute gradients w.r.t. input\n",
    "        return grad_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class SimpleDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, func, num_samples=50):\n",
    "        super(SimpleDataset, self).__init__()\n",
    "        self.num_samples = num_samples\n",
    "        xs = [random.uniform(-1, 1) for i in range(self.num_samples)]   # create random values for x\n",
    "        xys = [([x], [func(x)]) for x in xs]                            # compute y for each x\n",
    "        xys = [(np.asarray(x, dtype=np.float32),\n",
    "                np.asarray(y, dtype=np.float32)) for (x, y) in xys]     # convert to numpy array\n",
    "        self.samples = [(torch.from_numpy(x),            \n",
    "                         torch.from_numpy(y)) for (x, y) in xys]        # convert to torch tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to get and set DNN parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(dnn):\n",
    "    params = {}\n",
    "    params['ws'] = []\n",
    "    params['bs'] = []\n",
    "    for layer in dnn.layers:\n",
    "        if isinstance(layer, OurLinear):\n",
    "            params['ws'].append(layer.weight)\n",
    "            params['bs'].append(layer.bias)\n",
    "        elif isinstance(layer, nn.Linear):\n",
    "            params['ws'].append(layer.weight.item())\n",
    "            params['bs'].append(layer.bias.item())\n",
    "    return params\n",
    "\n",
    "def copy_params(py_dnn, our_dnn):\n",
    "    num_layers = len(py_dnn.layers)\n",
    "    for i in range(num_layers):\n",
    "        if isinstance(py_dnn.layers[i], nn.Linear):\n",
    "            our_dnn.layers[i].weight = py_dnn.layers[i].weight.item()\n",
    "            our_dnn.layers[i].bias = py_dnn.layers[i].bias.item()\n",
    "\n",
    "def format_params(params):\n",
    "    return 'params = {}'.format({k : [round(x, 3) for x in v] for k, v in params.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the function we want to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_function_we_want_to_learn(x):\n",
    "    return np.tanh(0.6 * x - 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize data loader and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_layers = 1\n",
    "lr = 0.01\n",
    "epoch_size = 50\n",
    "num_epochs = 10\n",
    "\n",
    "dataset = SimpleDataset(some_function_we_want_to_learn, num_samples = epoch_size * num_epochs)\n",
    "dataloader = DataLoader(dataset, shuffle=False, batch_size=1)\n",
    "\n",
    "py_dnn = PyDNN(num_layers)\n",
    "our_dnn = OurDNN(num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize both models to the same random start state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_params(py_dnn, our_dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train PyDNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[before training]\tparams = {'ws': [0.26], 'bs': [-0.911]}\n",
      "[after batch 50] loss = 0.000 params = {'ws': [0.296], 'bs': [-0.88]}\n",
      "[after batch 100] loss = 0.006 params = {'ws': [0.327], 'bs': [-0.856]}\n",
      "[after batch 150] loss = 0.001 params = {'ws': [0.35], 'bs': [-0.852]}\n",
      "[after batch 200] loss = 0.004 params = {'ws': [0.367], 'bs': [-0.845]}\n",
      "[after batch 250] loss = 0.001 params = {'ws': [0.387], 'bs': [-0.84]}\n",
      "[after batch 300] loss = 0.000 params = {'ws': [0.4], 'bs': [-0.839]}\n",
      "[after batch 350] loss = 0.003 params = {'ws': [0.418], 'bs': [-0.832]}\n",
      "[after batch 400] loss = 0.000 params = {'ws': [0.429], 'bs': [-0.838]}\n",
      "[after batch 450] loss = 0.002 params = {'ws': [0.445], 'bs': [-0.836]}\n",
      "[after batch 500] loss = 0.002 params = {'ws': [0.454], 'bs': [-0.843]}\n"
     ]
    }
   ],
   "source": [
    "py_params = get_params(py_dnn)\n",
    "py_loss = nn.MSELoss()\n",
    "py_opt = optim.SGD(py_dnn.parameters(), lr=lr)\n",
    "py_dnn.train()\n",
    "batch_idx = 0\n",
    "\n",
    "print('[before training]\\t{}'.format(format_params(py_params)))\n",
    "for _, (x, y) in enumerate(dataloader):\n",
    "    py_opt.zero_grad()              # in PyTorch the gradients are accumulated, so we zero them before each epoch\n",
    "    out = py_dnn(x)                 # forward pass over the network\n",
    "    loss = py_loss(out, y)          # compute the loss\n",
    "    loss.backward()                 # compute the gradients\n",
    "    py_opt.step()                   # update model parameters\n",
    "    batch_idx += 1\n",
    "    py_params = get_params(py_dnn)\n",
    "    if batch_idx % epoch_size == 0:\n",
    "        print('[after batch {}] loss = {:.3f} {}'.format(batch_idx, loss.item(), format_params(py_params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train OurDNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[before training]\tparams = {'ws': [0.26], 'bs': [-0.911]}\n",
      "[after batch 50] loss = 0.000 params = {'ws': [0.296], 'bs': [-0.88]}\n",
      "[after batch 100] loss = 0.006 params = {'ws': [0.327], 'bs': [-0.856]}\n",
      "[after batch 150] loss = 0.001 params = {'ws': [0.35], 'bs': [-0.852]}\n",
      "[after batch 200] loss = 0.004 params = {'ws': [0.367], 'bs': [-0.845]}\n",
      "[after batch 250] loss = 0.001 params = {'ws': [0.387], 'bs': [-0.84]}\n",
      "[after batch 300] loss = 0.000 params = {'ws': [0.4], 'bs': [-0.839]}\n",
      "[after batch 350] loss = 0.003 params = {'ws': [0.418], 'bs': [-0.832]}\n",
      "[after batch 400] loss = 0.000 params = {'ws': [0.429], 'bs': [-0.838]}\n",
      "[after batch 450] loss = 0.002 params = {'ws': [0.445], 'bs': [-0.836]}\n",
      "[after batch 500] loss = 0.002 params = {'ws': [0.454], 'bs': [-0.843]}\n"
     ]
    }
   ],
   "source": [
    "our_params = get_params(our_dnn)\n",
    "our_loss = OurMSELoss()\n",
    "batch_idx = 0\n",
    "\n",
    "print('[before training]\\t{}'.format(format_params(our_params)))\n",
    "for _, (x, y) in enumerate(dataloader):\n",
    "    out = our_dnn.forward(x.item())         # forward pass over the network\n",
    "    loss = our_loss.forward(out, y.item())  # compute the loss\n",
    "    grad = our_loss.backward()              # compute the gradients with respect to the model output\n",
    "    our_dnn.backward(grad, lr)              # compute the gradients and update model parameters\n",
    "    batch_idx += 1\n",
    "    our_params = get_params(our_dnn)\n",
    "    if batch_idx % epoch_size == 0:\n",
    "        print('[after batch {}] loss = {:.3f} {}'.format(batch_idx, loss, format_params(our_params)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
