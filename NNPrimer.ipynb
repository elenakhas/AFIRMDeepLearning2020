{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding up a Simple Neural Network from Scratch\n",
    "\n",
    "In this demo we will code up a simple neural network using (i) PyTorch and (ii) from scratch. Then we will demonstrate that both produce _identical_ outcome.\n",
    "\n",
    "\n",
    "## A simple DNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class OurLinear:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.w = random.uniform(-1, 1)\n",
    "        self.b = random.uniform(-1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x * self.w + self.b\n",
    "    \n",
    "    def backward(self, grad_out, lr):\n",
    "        grad_w = grad_out * self.x\n",
    "        grad_b = grad_out\n",
    "        grad_in = grad_out * self.w\n",
    "        self.w = self.w - lr * grad_w\n",
    "        self.b = self.b - lr * grad_b\n",
    "        return grad_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTanh:\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.y = np.tanh(x)\n",
    "        return self.y\n",
    "    \n",
    "    def backward(self, grad_out, lr):\n",
    "        grad_in = grad_out * (1 - self.y**2)\n",
    "        return grad_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurDNN:\n",
    "\n",
    "    def __init__(self, num_layers):\n",
    "        self.layers = []\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(OurLinear())\n",
    "            self.layers.append(OurTanh())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, grad_out, lr):\n",
    "        for layer in self.layers:\n",
    "            grad_out = layer.backward(grad_out, lr)\n",
    "        return grad_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple DNN in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class PyDNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers):\n",
    "        super(PyDNN, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Linear(1, 1))\n",
    "            layers.append(nn.Tanh())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple mean squared loss from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurMSELoss:\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return (y - x)**2\n",
    "    \n",
    "    def backward(self):\n",
    "        grad_in = -2 * (self.y - self.x)\n",
    "        return grad_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class SimpleDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, func, num_samples=50):\n",
    "        super(SimpleDataset, self).__init__()\n",
    "        self.num_samples = num_samples\n",
    "        xs = [random.uniform(-1, 1) for i in range(self.num_samples)]\n",
    "        self.samples = [(torch.from_numpy(np.asarray([x], dtype=np.float32)), torch.from_numpy(np.asarray([func(x)], dtype=np.float32))) for x in xs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to get and set DNN parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_py_params(py_dnn):\n",
    "    params = {}\n",
    "    params['ws'] = []\n",
    "    params['bs'] = []\n",
    "    for name, param in py_dnn.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if name.endswith('weight'):\n",
    "                params['ws'].append(param.data[0].item())\n",
    "            elif name.endswith('bias'):\n",
    "                params['bs'].append(param.data[0].item())\n",
    "    return params\n",
    "\n",
    "def get_our_params(our_dnn):\n",
    "    params = {}\n",
    "    params['ws'] = []\n",
    "    params['bs'] = []\n",
    "    for layer in our_dnn.layers:\n",
    "        if isinstance(layer, OurLinear):\n",
    "            params['ws'].append(layer.w)\n",
    "            params['bs'].append(layer.b)\n",
    "    return params\n",
    "\n",
    "def format_params(params):\n",
    "    return 'params = {}'.format({k : [round(x, 3) for x in v] for k, v in params.items()})\n",
    "\n",
    "def copy_params(py_dnn, our_dnn):\n",
    "    py_params = get_py_params(py_dnn)\n",
    "    for i in range(len(py_params['ws'])):\n",
    "        our_dnn.layers[i].w = py_params['ws'][i]\n",
    "        our_dnn.layers[i].b = py_params['bs'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the function we want to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_function_we_want_to_learn(x):\n",
    "    return np.tanh(-0.6 * x - 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_layers = 1\n",
    "lr = 0.01\n",
    "epoch_size = 50\n",
    "num_epochs = 10\n",
    "\n",
    "dataset = SimpleDataset(some_function_we_want_to_learn, num_samples = epoch_size * num_epochs)\n",
    "dataloader = DataLoader(dataset, shuffle=False, batch_size=1)\n",
    "\n",
    "py_dnn = PyDNN(num_layers)\n",
    "our_dnn = OurDNN(num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize both models to the same random start state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize to the same starting parameters\n",
    "copy_params(py_dnn, our_dnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train PyDNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[before training]\tparams = {'ws': [-0.157], 'bs': [-0.955]}\n",
      "[after batch 50] loss = 0.002 params = {'ws': [-0.2], 'bs': [-0.916]}\n",
      "[after batch 100] loss = 0.001 params = {'ws': [-0.24], 'bs': [-0.886]}\n",
      "[after batch 150] loss = 0.004 params = {'ws': [-0.262], 'bs': [-0.875]}\n",
      "[after batch 200] loss = 0.002 params = {'ws': [-0.29], 'bs': [-0.861]}\n",
      "[after batch 250] loss = 0.007 params = {'ws': [-0.319], 'bs': [-0.843]}\n",
      "[after batch 300] loss = 0.021 params = {'ws': [-0.345], 'bs': [-0.833]}\n",
      "[after batch 350] loss = 0.009 params = {'ws': [-0.364], 'bs': [-0.831]}\n",
      "[after batch 400] loss = 0.002 params = {'ws': [-0.383], 'bs': [-0.828]}\n",
      "[after batch 450] loss = 0.013 params = {'ws': [-0.398], 'bs': [-0.826]}\n",
      "[after batch 500] loss = 0.006 params = {'ws': [-0.412], 'bs': [-0.827]}\n"
     ]
    }
   ],
   "source": [
    "py_params = get_py_params(py_dnn)\n",
    "py_loss = nn.MSELoss()\n",
    "py_opt = optim.SGD(py_dnn.parameters(), lr=lr)\n",
    "py_dnn.train()\n",
    "batch_idx = 0\n",
    "print('[before training]\\t{}'.format(format_params(py_params)))\n",
    "for _, (x, y) in enumerate(dataloader):\n",
    "    py_opt.zero_grad()\n",
    "    out = py_dnn(x)\n",
    "    loss = py_loss(out, y)\n",
    "    loss.backward()\n",
    "    py_opt.step()\n",
    "    batch_idx += 1\n",
    "    py_params = get_py_params(py_dnn)\n",
    "    if batch_idx % epoch_size == 0:\n",
    "        print('[after batch {}] loss = {:.3f} {}'.format(batch_idx, loss.item(), format_params(py_params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the OurDNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[before training]\tparams = {'ws': [-0.157], 'bs': [-0.955]}\n",
      "[after batch 50] loss = 0.002 params = {'ws': [-0.235], 'bs': [-0.898]}\n",
      "[after batch 100] loss = 0.001 params = {'ws': [-0.298], 'bs': [-0.872]}\n",
      "[after batch 150] loss = 0.002 params = {'ws': [-0.333], 'bs': [-0.87]}\n",
      "[after batch 200] loss = 0.001 params = {'ws': [-0.37], 'bs': [-0.866]}\n",
      "[after batch 250] loss = 0.003 params = {'ws': [-0.405], 'bs': [-0.857]}\n",
      "[after batch 300] loss = 0.009 params = {'ws': [-0.435], 'bs': [-0.857]}\n",
      "[after batch 350] loss = 0.004 params = {'ws': [-0.457], 'bs': [-0.862]}\n",
      "[after batch 400] loss = 0.000 params = {'ws': [-0.476], 'bs': [-0.864]}\n",
      "[after batch 450] loss = 0.005 params = {'ws': [-0.491], 'bs': [-0.865]}\n",
      "[after batch 500] loss = 0.002 params = {'ws': [-0.504], 'bs': [-0.869]}\n"
     ]
    }
   ],
   "source": [
    "our_params = get_our_params(our_dnn)\n",
    "our_loss = OurMSELoss()\n",
    "batch_idx = 0\n",
    "print('[before training]\\t{}'.format(format_params(our_params)))\n",
    "for _, (x, y) in enumerate(dataloader):\n",
    "    out = our_dnn.forward(x.item())\n",
    "    loss = our_loss.forward(out, y.item())\n",
    "    grad = our_loss.backward()\n",
    "    our_dnn.backward(grad, lr)\n",
    "    batch_idx += 1\n",
    "    our_params = get_our_params(our_dnn)\n",
    "    if batch_idx % epoch_size == 0:\n",
    "        print('[after batch {}] loss = {:.3f} {}'.format(batch_idx, loss, format_params(our_params)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
