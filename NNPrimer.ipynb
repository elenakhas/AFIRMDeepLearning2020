{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import datetime\n",
    "import operator\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "def print_message(s):\n",
    "    print(\"[{}] {}\".format(datetime.datetime.now().strftime(\"%b %d, %H:%M:%S\"), s), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurFunc:\n",
    "\n",
    "    def __init__(self, num_layers, params = None):\n",
    "        self.num_layers = num_layers\n",
    "        if params == None:\n",
    "            self.ws = [random.uniform(-1, 1) for i in range(num_layers)]\n",
    "            self.bs = [random.uniform(-1, 1) for i in range(num_layers)]\n",
    "        else:\n",
    "            assert len(params['ws']) == num_layers and len(params['bs']) == num_layers\n",
    "            self.ws = params['ws']\n",
    "            self.bs = params['bs']\n",
    "        self.layer_outputs = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.layer_outputs.clear()\n",
    "        self.layer_outputs.append(x)\n",
    "        for i in range(self.num_layers):\n",
    "            x = np.tanh(x * self.ws[i] + self.bs[i])\n",
    "            self.layer_outputs.append(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, y, lr):\n",
    "        grads = [0 for i in range(self.num_layers)]\n",
    "        grads_w = [0 for i in range(self.num_layers)]\n",
    "        grads_b = [0 for i in range(self.num_layers)]\n",
    "        grads[self.num_layers - 1] = -2 * (y - self.layer_outputs[self.num_layers])\n",
    "        for i in range(self.num_layers - 2, - 1, -1):\n",
    "            grads[i] = grads[i + 1] * (1 - self.layer_outputs[i + 2]**2) * self.ws[i + 1]\n",
    "        for i in range(self.num_layers):\n",
    "            grads_w[i] = grads[i] * (1 - self.layer_outputs[i + 1]**2) * self.layer_outputs[i]\n",
    "            grads_b[i] = grads[i] * (1 - self.layer_outputs[i + 1]**2)\n",
    "        for i in range(self.num_layers):\n",
    "            self.ws[i] -= lr * grads_w[i]\n",
    "            self.bs[i] -= lr * grads_b[i]\n",
    "        self.layer_outputs.clear()\n",
    "        return {'ws' : grads_w, 'bs' : grads_b}\n",
    "\n",
    "    def get_params(self):\n",
    "        return {'ws' : self.ws, 'bs' : self.bs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNFunc(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers):\n",
    "        super(DNNFunc, self).__init__()\n",
    "        layers = [[nn.Linear(1, 1), nn.Tanh()] for i in range(num_layers)]\n",
    "        layers = [layer for layer_groups in layers for layer in layer_groups]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_params(self):\n",
    "        params = {}\n",
    "        params['ws'] = []\n",
    "        params['bs'] = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if name.endswith('weight'):\n",
    "                    params['ws'].append(param.data[0].item())\n",
    "                elif name.endswith('bias'):\n",
    "                    params['bs'].append(param.data[0].item())\n",
    "        return params\n",
    "    \n",
    "    def get_grads(self):\n",
    "        grads = {}\n",
    "        grads['ws'] = []\n",
    "        grads['bs'] = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if name.endswith('weight'):\n",
    "                    grads['ws'].append(param.grad.item())\n",
    "                elif name.endswith('bias'):\n",
    "                    grads['bs'].append(param.grad.item())\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, true_func):\n",
    "        super(SimpleDataset, self).__init__()\n",
    "        self.num_samples = 500\n",
    "        xs = [random.uniform(-1, 1) for i in range(self.num_samples)]\n",
    "        self.samples = [(torch.from_numpy(np.asarray([x], dtype=np.float32)), torch.from_numpy(np.asarray([true_func.forward(x)], dtype=np.float32))) for x in xs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true params =                               {'bs': [0.766], 'ws': [-0.316]}\n",
      "[before training]              dnn params = {'bs': [-0.4], 'ws': [0.366]} our params = {'bs': [-0.4], 'ws': [0.366]} \n",
      "[after batch 50] loss = 0.001 dnn params = {'bs': [0.732], 'ws': [-0.207]} our params = {'bs': [0.732], 'ws': [-0.207]}\n",
      "[after batch 100] loss = 0.000 dnn params = {'bs': [0.746], 'ws': [-0.279]} our params = {'bs': [0.746], 'ws': [-0.279]}\n",
      "[after batch 150] loss = 0.000 dnn params = {'bs': [0.763], 'ws': [-0.303]} our params = {'bs': [0.763], 'ws': [-0.303]}\n",
      "[after batch 200] loss = 0.000 dnn params = {'bs': [0.765], 'ws': [-0.311]} our params = {'bs': [0.765], 'ws': [-0.311]}\n",
      "[after batch 250] loss = 0.000 dnn params = {'bs': [0.766], 'ws': [-0.314]} our params = {'bs': [0.766], 'ws': [-0.314]}\n",
      "[after batch 300] loss = 0.000 dnn params = {'bs': [0.766], 'ws': [-0.315]} our params = {'bs': [0.766], 'ws': [-0.315]}\n",
      "[after batch 350] loss = 0.000 dnn params = {'bs': [0.766], 'ws': [-0.316]} our params = {'bs': [0.766], 'ws': [-0.316]}\n",
      "[after batch 400] loss = 0.000 dnn params = {'bs': [0.766], 'ws': [-0.316]} our params = {'bs': [0.766], 'ws': [-0.316]}\n",
      "[after batch 450] loss = 0.000 dnn params = {'bs': [0.766], 'ws': [-0.316]} our params = {'bs': [0.766], 'ws': [-0.316]}\n",
      "[after batch 500] loss = 0.000 dnn params = {'bs': [0.766], 'ws': [-0.316]} our params = {'bs': [0.766], 'ws': [-0.316]}\n",
      "[after batch 550] loss = 0.000 dnn params = {'bs': [0.766], 'ws': [-0.316]} our params = {'bs': [0.766], 'ws': [-0.316]}\n",
      "[after batch 600] loss = 0.000 dnn params = {'bs': [0.766], 'ws': [-0.316]} our params = {'bs': [0.766], 'ws': [-0.316]}\n",
      "[after batch 650] loss = 0.000 dnn params = {'bs': [0.766], 'ws': [-0.316]} our params = {'bs': [0.766], 'ws': [-0.316]}\n",
      "[after batch 700] loss = 0.000 dnn params = {'bs': [0.766], 'ws': [-0.316]} our params = {'bs': [0.766], 'ws': [-0.316]}\n",
      "[after batch 750] loss = 0.000 dnn params = {'bs': [0.766], 'ws': [-0.316]} our params = {'bs': [0.766], 'ws': [-0.316]}\n",
      "[after batch 800] loss = 0.000 dnn params = {'bs': [0.766], 'ws': [-0.316]} our params = {'bs': [0.766], 'ws': [-0.316]}\n",
      "[after batch 850] loss = 0.000 dnn params = {'bs': [0.766], 'ws': [-0.316]} our params = {'bs': [0.766], 'ws': [-0.316]}\n",
      "[after batch 900] loss = 0.000 dnn params = {'bs': [0.766], 'ws': [-0.316]} our params = {'bs': [0.766], 'ws': [-0.316]}\n",
      "[after batch 950] loss = 0.000 dnn params = {'bs': [0.766], 'ws': [-0.316]} our params = {'bs': [0.766], 'ws': [-0.316]}\n",
      "[after batch 1000] loss = 0.000 dnn params = {'bs': [0.766], 'ws': [-0.316]} our params = {'bs': [0.766], 'ws': [-0.316]}\n"
     ]
    }
   ],
   "source": [
    "num_layers = 1\n",
    "lr = 0.1\n",
    "\n",
    "true_func = OurFunc(num_layers)\n",
    "true_params = {k : [round(x, 3) for x in v] for k, v in true_func.get_params().items()}\n",
    "print('true params =                              ', true_params)\n",
    "\n",
    "dnn_func = DNNFunc(num_layers)\n",
    "our_func = OurFunc(num_layers, params = dnn_func.get_params())\n",
    "\n",
    "dnn_params = {k : [round(x, 3) for x in v] for k, v in dnn_func.get_params().items()}\n",
    "our_params = {k : [round(x, 3) for x in v] for k, v in our_func.get_params().items()}\n",
    "print('[before training]              dnn params = {} our params = {} '.format(dnn_params, our_params))\n",
    "\n",
    "dataset = SimpleDataset(true_func)\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(dnn_func.parameters(), lr=lr)\n",
    "dnn_func.train()\n",
    "batch_idx = 0\n",
    "for _, batch in enumerate(dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    xs = batch[0]\n",
    "    ys = batch[1]\n",
    "    out = dnn_func(xs)\n",
    "    loss = criterion(out, ys)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    our_func.forward(xs.item())\n",
    "    our_grads = our_func.backward(ys.item(), lr)        \n",
    "    batch_idx += 1\n",
    "    if batch_idx % 50 == 0:\n",
    "        dnn_params = {k : [round(x, 3) for x in v] for k, v in dnn_func.get_params().items()}\n",
    "        our_params = {k : [round(x, 3) for x in v] for k, v in our_func.get_params().items()}\n",
    "        print('[after batch {}] loss = {:.3f} dnn params = {} our params = {}'.format(batch_idx, loss.item(), dnn_params, our_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
